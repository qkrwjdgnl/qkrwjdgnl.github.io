---
title: "High-Resolution Image Synthesis with Latent Diffusion Models (I)"
classes: wide
---

<figure align="center">
  <img src="{{ '/assets/image/LDM/0.png' | relative_url }}" width="50%">
</figure>

# 기초부터 이해하는 Stable Diffusion (I): 어떻게 구성되는가

몇 달 전, AIGC가 대유행을 일으키면서 고화질의 생성 이미지가 계속해서 등장했고, 그 중에서도 가장 중요한 오픈소스 모델인 Stable Diffusion이 기술적, 상업적으로 큰 인기를 끌었습니다. 이 모델은 빠른 속도로 지속적으로 발전하고 있습니다. 이전에 관련 지식이 없었던 초보자로서, 관련 기술 지식을 이해하기 위해 많은 글을 찾아보았고, 마침내 Jay Alammar의 글이 가장 이해하기 쉽다는 것을 발견하였습니다. 그래서 이 글을 간단히 번역하기로 결정했습니다. 이를 통해 더 많은 사람들이 이 강력한 기술을 처음부터 이해할 수 있게 돕고자 합니다.

원문이 길기 때문에, 여기서는 세 편의 글로 나누어 설명하겠습니다:

1. 첫 번째 편, 이 글에서는 '**무엇인가**'에 대해 주로 다룹니다. 여기에는 Stable Diffusion이 무엇인지, 그 안의 각 모듈이 무엇인지가 포함됩니다.
2. 두 번째 편, '**어떻게 하는가**'에 대해 다룹니다. 즉, Diffusion이 어떻게 훈련되고 사용되는지에 대한 문제입니다.
3. 세 번째 편, '**어떻게 하는가**'에 대해 다룹니다. 구체적으로, 의미 정보가 생성 이미지 과정에 어떻게 영향을 미치는지에 대해 설명합니다.

이제 본격적으로 첫 번째 글의 소개에 들어가서, Stable Diffusion이 무엇인지, 그리고 그 안의 일부 모듈이 무엇인지에 대해 이야기해 보겠습니다.

> 원문 링크: [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)<br>
> 저자: Jay Alammar<br>
> 번역자: Zhenghui Piao

AI 이미지 생성이 최근에 보여준 잠재력은 정말 놀랍습니다. 단순한 텍스트 설명에서 시작하여 마법처럼 고품질의 이미지를 만들어냅니다. 이는 인간의 창작 방식을 깊이 있게 확장한 것이라고 할 수 있습니다. 그중에서도 Stable Diffusion의 공개는 마일스톤이라 할 수 있는데, 그것의 오픈소스는 대중에게 매우 높은 품질의 모델을 제공하는 것을 의미할 뿐만 아니라, 이 모델은 **빠른 실행 속도**와 낮은 **메모리 요구 사항**을 유지하면서도 인상적인 성능을 발휘합니다.

Diffusion이라는 놀라운 기술을 사용한 후, 왜 이렇게 훌륭한 결과를 낼 수 있는지 궁금해할 수 있습니다. 여기서 가능한 한 간단하고 명확한 설명을 드리겠습니다.

Stable Diffusion 모델은 실로 다재다능하며, 텍스트에서 이미지 생성, 이미지에서 이미지 생성, 특정 캐릭터의 묘사, 심지어 이미지 고해상도화(super-resolution) 또는 이미지 복원(Inpainting)과 같은 여러 작업을 탁월하게 수행할 수 있습니다. 하지만 가장 기본적인 소개를 하는 첫 번째 글에서는 가장 기본적인 **"텍스트에서 이미지 생성(text-to-image)"** 모듈, 즉 txt2img 부분에 집중해서 설명하겠습니다. 아래 그림은 기본적인 텍스트에서 이미지 생성의 예시를 보여주며, 입력된 "천국(paradise), 광활한(cosmic), 해변(beach)"에 대해 볼 수 있듯이, 가장 오른쪽에 생성된 이미지는 입력 요구 사항을 잘 충족합니다. 그림 속에는 푸른 하늘과 구름은 물론, 광활한 해변도 끝없이 펼쳐져 있습니다:

<!-- <center><img src='{{"/assets/image/LDM/1.png" | relative_url}}' width="50%"></center> -->
<figure align="center">
  <img src="{{ '/assets/image/LDM/1.png' | relative_url }}" width="50%">
  <figcaption style="color: grey;">가장 간단한 txt2img 개념도. 이후 우리는 이 그림 속 txt2img 과정을 지속적으로 세분화하고 분해해 나갈 것.</figcaption>
</figure>

비록 이 글에서 이미지에서 이미지로 변환하는 모듈(이른바 img2img)에 대해 아직 설명하지 않았지만, 이 모듈의 개념도도 잠시 소개하겠습니다. 아래 그림과 같이, 이번에 입력은 단순한 '텍스트'에서 '이미지+텍스트' 형태로 변했으며, 생성된 결과는 원본 이미지와 텍스트 프롬프트의 조합으로 결정됩니다. 이번 입력은 '해적선(pirate ship)'과 위의 img2img에서 생성된 이미지로, 최종 출력 결과는 실제로 입력 이미지의 배를 해적선으로 변환했습니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/2.png' | relative_url }}" width="50%">
  <figcaption style="color: grey;">img2img 개념도에서, 입력은 '해적선(pirate ship)'이며, 최종 출력 결과는 실제로 입력된 이미지의 선박을 해적선으로 변환.</figcaption>
</figure>

이제, 이 기술이 가진 배경에 있는 원리를 정식으로 알아보도록 합시다.

## 1. 구성 모듈

Stable Diffusion은 사실 상당히 **복합적인** 시스템으로, 다양한 모델 모듈들이 포함되어 있습니다. 그렇다면 처음 마주치게 되는 문제는, 인간이 이해하는 **텍스트**를 컴퓨터가 이해하는 **수학적 언어**로 어떻게 변환할 것인가 입니다. 결국 컴퓨터는 영어를 이해하지 못하니까요. 이때 'text understander'가 필요한데, 그것은 이미지를 생성하기 전에, 아래 그림에서 보이는 **푸른색 'text understander'**가 텍스트를 컴퓨터가 이해할 수 있는 어떤 수학적 표현으로 변환하는 작업을 돕습니다:

<figure align="center">
  <img src="{{ '/assets/image/LDM/3.png' | relative_url }}" width="50%">
  <figcaption style="color: grey;">푸른색의 text understander(즉, 텍스트의 encoder 인코더)는 인간의 언어를 컴퓨터가 이해할 수 있는 의미 있는 내용으로 변환.</figcaption>
</figure>

우리는 이후 세 번째 글에서 이 text understander가 실제로 텍스트를 어떻게 이해하고 어떻게 훈련되는지에 대해 설명할 것입니다. 하지만 지금은 이 부분을 잠시 넘어가고, 이 text understander가 특별한 Transformer 언어 모델이라는 것만 알면 됩니다. 이 모델의 입력은 인간의 언어이고, 출력은 일련의 **벡터**로, 이 벡터들은 우리가 입력한 텍스트의 의미를 담고 있습니다.

그러면 이제, 의미를 대표하는 벡터(예를 들어 아래의 파란색 3x5 격자)를 가지게 되었으니, 이 의미 벡터를 실제 이미지 생성기, 즉 아래 그림의 **분홍색 Image Generator**에 넘겨줍니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/4.png' | relative_url }}" width="50%">
  <figcaption style="color: grey;">파란색 격자의 의미 벡터가 분홍색 이미지 생성기로 입력되어 이미지 생성을 정식으로 시작. </figcaption>
</figure>

이 분홍색 이미지 생성기(Image Generator)는 **두 개의 하위 모듈**로 분해하여 볼 수 있습니다.

### 1.1 Image information creator

아래 그림에서 보이는 이 분홍색 모듈은 Stable Diffusion의 비밀 무기이며, 다른 diffusion 모델과의 가장 큰 차이점입니다. 많은 성능 향상이 바로 여기서 비롯됩니다.

먼저 명확히 해야 할 점은, **이미지 정보 생성기는 직접적으로 이미지를 생성하지 않고**, 오히려 낮은 차원의 **이미지 정보**, 즉 이른바 **'잠재 공간 정보(information of latent space)'**를 생성합니다. 이 잠재 공간 정보는 아래 흐름도에서 보이는 그 분홍색 4x3 격자로 나타나며, 이후 아래 그림의 이 잠재 공간 정보를 아래 그림의 노란색 디코더(Decoder)에 입력하여 이미지를 성공적으로 생성할 수 있습니다. Stable Diffusion이 주로 인용하는 논문 'latent diffusion'의 'latent'도 잠재 변수에서 유래한 **'잠재(latent)'**에서 온 것입니다.

일반적인 diffusion 모델은 직접 이미지를 생성합니다. 잠재 변수를 먼저 생성하는 과정은 없기 때문에, 보통의 diffusion은 이 단계에서 생성해야 하는 정보가 더 많고, 부담도 큽니다. 그래서 이전의 diffusion 모델은 속도나 자원 활용 면에서 Stable Diffusion보다 떨어집니다. 그렇다면 기술적으로, 이 이미지 잠재 변수는 어떻게 생성되는 것일까요? 이는 **Unet**과 **Schedule 알고리즘**에 의해 공동으로 완성됩니다. Schedule 알고리즘은 **생성 과정의 진행을 제어**하고, Unet은 **생성 과정을 단계별로 구체적으로 수행**합니다. Stable Diffusion에서는 전체 Unet의 생성 반복 과정이 대략 50~100회 정도 반복되며, 이 반복 과정에서 잠재 변수의 품질이 지속적으로 개선됩니다. 아래 그림에서 분홍색 Image Information Creator의 왼쪽 하단에 있는 순환 표시는 이 반복 과정을 상징합니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/5.png' | relative_url }}" width="50%">
</figure>

### 1.2 Image Decoder

이미지 디코더는 우리가 위에서 언급한 디코더(Decoder)로, 이미지 정보 생성기(Image Information Creator)로부터 **이미지 정보의 잠재 변수**를 인수받아 이를 차원 확대(upscale)하여 완전한 이미지로 복원합니다. 이미지 디코더는 마지막 단계에서만 작동하며, 실제로 완성된 이미지를 얻을 수 있는 최종 과정입니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/6.png' | relative_url }}" width="50%">
</figure>

위에서는 Stable Diffusion의 각 모듈의 기능에 대해 간략하게 이야기했습니다. 이제 이 시스템에서 입력과 출력 벡터의 형태를 좀 더 구체적으로 살펴보면서 Stable Diffusion의 작동 원리에 대해 더 직관적으로 이해할 수 있을 것입니다:

- **Clip Text** for text encoding
    Input: text.
    Output: 77 token embeddings vectors, each in 768 dimensions.

- **UNet + Scheduler** to gradually process/diffuse information in the information (latent) space.
    Input: text embeddings and a starting multi-dimensional array (structured lists of numbers, also called a tensor) made up of noise.
    Output: A processed information array

- **Autoencoder Decoder** that paints the final image using the processed information array.
    Input: The processed information array (dimensions: (4,64,64))
    Output: The resulting image (dimensions: (3, 512, 512) which are (red/green/blue, width, height))

<figure align="center">
  <img src="{{ '/assets/image/LDM/7.png' | relative_url }}" width="50%">
</figure>

## 2. Diffusion(확산)은 도대체 무슨 의미일까?

Diffusion 모델은 한국어로 '확산 모델'로 번역되며, 그렇다면 이 '확산'은 구체적으로 어디에서 나타나는 것일까요? 이것이 바로 우리가 이 두 번째 파트에서 집중적으로 설명할 과정입니다. 먼저 random 함수를 사용해 잠재 변수 크기의 **순수 노이즈**(아래 그림의 왼쪽 아래 투명한 4x4)를 생성합니다. 확산 과정은 Image Information Creator 내에서 일어나며, 초기의 **순수 노이즈**(아래 그림의 왼쪽 아래 투명한 4x4)와 **의미 벡터**(아래 그림의 왼쪽 상단 파란색 3x5)를 가지고 나서, unet은 의미 벡터와 결합하여 노이즈 잠재 변수에서 **노이즈를 지속적으로 제거**하며, 약 50~100회 정도 반복하여 노이즈를 완전히 제거하고, 동시에 잠재 변수에 의미 정보를 지속적으로 주입하여, 의미가 있는 잠재 변수(아래 그림의 분홍색 4x4)를 얻게 됩니다. 또한 scheduler가 있어서 unet의 노이즈 제거 강도를 제어하며 전체 노이즈 제거 과정을 조정합니다. scheduler는 노이즈를 제거하는 다양한 단계에서 동적으로 제거 강도를 조정할 수 있으며, 특정한 작업에서는 일정한 속도로 노이즈를 제거하는 것도 가능합니다. 이는 우리가 처음에 설계할 때에 달려 있습니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/8.png' | relative_url }}" width="50%">
</figure>

이 확산 과정은 단계별로 노이즈를 제거하는 반복적인 과정으로, 각 단계마다 잠재 변수에 의미 정보를 주입하며, 노이즈 제거가 완료될 때까지 계속 반복합니다. 직관적인 이해를 위해 초기의 순수 노이즈(아래 그림의 왼쪽 상단 투명한 4x4)와 최종의 노이즈가 제거된 잠재 변수(아래 그림의 오른쪽 상단 분홍색 4x4)를 **모두 최종의 Image Decoder**를 통해 어떤 이미지가 나오는지 볼 수 있습니다. 예상대로 순수 노이즈 자체에는 어떠한 유효 정보도 없기 때문에, 디코딩된 이미지 역시 순수 노이즈일 것이고, 아래 그림 왼쪽에 표시된 것처럼 보일 것입니다. 반면에 최종의 노이즈가 제거된 잠재 변수는 이미 의미 정보와 결합되었기 때문에, 결국 디코딩된 이미지는 의미 정보가 포함된 유효한 이미지가 될 것이며, 아래 그림 오른쪽에 표시된 이미지처럼 보일 것입니다.

<figure align="center">
  <img src="{{ '/assets/image/LDM/9.png' | relative_url }}" width="50%">
</figure>

우리는 앞서 확산 과정이 **여러 차례 반복**되는 과정이라고 언급한 바 있습니다. 각 반복 단계에서의 입력은 하나의 잠재 변수이며, 출력도 잠재 변수입니다. 다만, 출력된 잠재 변수는 더 적은 노이즈와 더 많은 의미 정보를 갖습니다. 아래 그림의 4x4 잠재 변수가 투명에서 분홍색으로 변하는 과정은 이 반복 과정을 대표합니다. 색이 더 분홍색이 될수록 반복 횟수가 많아지고, 노이즈도 더 줄어듭니다

<figure align="center">
  <img src="{{ '/assets/image/LDM/10.png' | relative_url }}" width="50%">
</figure>

이때 우리가 Image Decoder를 몰래 사용하여 각 단계에서 해당하는 이미지를 미리 보면, 우리가 원하는 이미지가 노이즈에서 점차 탄생하는 전 과정을 볼 수 있습니다:

<figure align="center">
  <img src="{{ '/assets/image/LDM/11.png' | relative_url }}" width="50%">
</figure>

이것은 마법 같은 과정입니다. 아래 동영상은 반복적인 노이즈 제거 과정을 보여줍니다. 동영상을 보며 과정을 확인해 보겠습니다:

<video width="320" height="240" controls>
  <source src="/assets/image/LDM/1.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

## 3. Conclusion

이로써, 우리는 Stable Diffusion이 무엇인지, 그리고 그 안의 다양한 모듈이 무엇인지, 심지어 그 작동 과정을 간단히 살펴보았습니다. Diffusion이 어떻게 훈련되고, 어떻게 제어되는지에 대해서는 글의 길이를 고려하여 후속 글에서 자세히 다루도록 하겠습니다. 여기에서 마지막으로 간단히 요약하자면:

1. 첫 번째 부분에서는 Stable Diffusion의 **주요 모듈**들을 소개했습니다. 여기에는 텍스트를 이해하는 **Text Understander**, 이미지의 잠재 변수를 생성하는 **Image Information Creator**, 그리고 이 잠재 변수를 이용해 실제 이미지를 생성하는 **Image Decoder**가 포함됩니다.

2. 또한, Diffusion이 **이미지를 생성하는 과정**을 소개했습니다. 이 과정에서는 시스템 내에서 **벡터 형태**가 거치는 일련의 변화와 각 단계에서 이미지 잠재 변수가 디코딩된 후의 **시각화**를 포함합니다.

다음 글은 여기를 클릭하여 읽을 수 있습니다~

